{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed2c3a4",
   "metadata": {},
   "source": [
    "# Data Science Interview Preparation Code: Pandas & Scikit-learn\n",
    "\n",
    "This notebook compiles essential interview questions and solutions for Pandas and Scikit-learn, suitable for data scientists with 3 years of experience. It covers data manipulation, preprocessing, model training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47029898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab77727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "    OrderID  CustomerID  OrderDate   Product  Quantity  Price\n",
      "0         1         101 2024-01-15    Laptop         1   1200\n",
      "1         2         102 2024-01-20     Mouse         2     25\n",
      "2         3         101 2024-02-01  Keyboard         1     75\n",
      "3         4         103 2024-02-05   Monitor         1    300\n",
      "4         5         102 2024-02-10    Laptop         1   1150\n",
      "5         6         104 2024-03-01     Mouse         3     20\n",
      "6         7         101 2024-03-05    Laptop         2   1250\n",
      "7         8         103 2024-03-10  Keyboard         1     80\n",
      "8         9         105 2024-03-15   Monitor         1    320\n",
      "9        10         104 2024-04-01     Mouse         2     22\n",
      "10        1         101 2024-01-15    Laptop         1   1200\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'OrderID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1],\n",
    "    'CustomerID': [101, 102, 101, 103, 102, 104, 101, 103, 105, 104, 101],\n",
    "    'OrderDate': pd.to_datetime(['2024-01-15', '2024-01-20', '2024-02-01', '2024-02-05', '2024-02-10',\n",
    "                                  '2024-03-01', '2024-03-05', '2024-03-10', '2024-03-15', '2024-04-01', '2024-01-15']),\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Laptop',\n",
    "                'Mouse', 'Laptop', 'Keyboard', 'Monitor', 'Mouse', 'Laptop'],\n",
    "    'Quantity': [1, 2, 1, 1, 1, 3, 2, 1, 1, 2, 1],\n",
    "    'Price': [1200, 25, 75, 300, 1150, 20, 1250, 80, 320, 22, 1200]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdd005a",
   "metadata": {},
   "source": [
    "## Question 1: Data Inspection\n",
    "\n",
    "Task: Display the first 5 rows of the DataFrame, its column names and their data types, and check for any missing values across all columns.\n",
    "\n",
    "Expected Output: df.head(), df.info(), df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a454c252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Head of DataFrame:\n",
      "   OrderID  CustomerID  OrderDate   Product  Quantity  Price\n",
      "0        1         101 2024-01-15    Laptop         1   1200\n",
      "1        2         102 2024-01-20     Mouse         2     25\n",
      "2        3         101 2024-02-01  Keyboard         1     75\n",
      "3        4         103 2024-02-05   Monitor         1    300\n",
      "4        5         102 2024-02-10    Laptop         1   1150\n",
      "\n",
      "Q1: DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11 entries, 0 to 10\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   OrderID     11 non-null     int64         \n",
      " 1   CustomerID  11 non-null     int64         \n",
      " 2   OrderDate   11 non-null     datetime64[ns]\n",
      " 3   Product     11 non-null     object        \n",
      " 4   Quantity    11 non-null     int64         \n",
      " 5   Price       11 non-null     int64         \n",
      "dtypes: datetime64[ns](1), int64(4), object(1)\n",
      "memory usage: 660.0+ bytes\n",
      "\n",
      "Q1: Missing values:\n",
      "OrderID       0\n",
      "CustomerID    0\n",
      "OrderDate     0\n",
      "Product       0\n",
      "Quantity      0\n",
      "Price         0\n",
      "dtype: int64\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Q1: Head of DataFrame:\")\n",
    "print(df.head())\n",
    "print(\"\\nQ1: DataFrame Info:\")\n",
    "df.info()\n",
    "print(\"\\nQ1: Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c7f7ef",
   "metadata": {},
   "source": [
    "## Question 2: Create a New Column (Total Price)\n",
    "\n",
    "Task: Add a new column named TotalPrice to the DataFrame, which is calculated as Quantity * Price.\n",
    "\n",
    "Expected Output: DataFrame with the new TotalPrice column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e7227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with TotalPrice column:\n",
      "    OrderID  CustomerID  OrderDate   Product  Quantity  Price  TotalPrice\n",
      "0         1         101 2024-01-15    Laptop         1   1200        1200\n",
      "1         2         102 2024-01-20     Mouse         2     25          50\n",
      "2         3         101 2024-02-01  Keyboard         1     75          75\n",
      "3         4         103 2024-02-05   Monitor         1    300         300\n",
      "4         5         102 2024-02-10    Laptop         1   1150        1150\n",
      "5         6         104 2024-03-01     Mouse         3     20          60\n",
      "6         7         101 2024-03-05    Laptop         2   1250        2500\n",
      "7         8         103 2024-03-10  Keyboard         1     80          80\n",
      "8         9         105 2024-03-15   Monitor         1    320         320\n",
      "9        10         104 2024-04-01     Mouse         2     22          44\n",
      "10        1         101 2024-01-15    Laptop         1   1200        1200\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "df['TotalPrice'] = df['Quantity'] * df['Price']\n",
    "print(\"DataFrame with TotalPrice column:\")\n",
    "print(df)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4638c52",
   "metadata": {},
   "source": [
    "## Question 3: Filtering Data\n",
    "\n",
    "Task: Filter the DataFrame to show only orders where the Product is 'Laptop' and the Quantity is greater than 1.\n",
    "\n",
    "Expected Output: Filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e2b8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame:\n",
      "   OrderID  CustomerID  OrderDate Product  Quantity  Price  TotalPrice\n",
      "6        7         101 2024-03-05  Laptop         2   1250        2500\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df[(df['Product'] == 'Laptop') & (df['Quantity'] > 1)]\n",
    "print(\"Filtered DataFrame:\")\n",
    "print(filtered_df)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194d60b",
   "metadata": {},
   "source": [
    "## Question 4: Grouping and Aggregation\n",
    "\n",
    "Task: Calculate the total TotalPrice for each CustomerID.\n",
    "\n",
    "Expected Output: A Series or DataFrame showing CustomerID and their aggregated TotalPrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328ec1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped DataFrame by CustomerID:\n",
      "   CustomerID  TotalPrice\n",
      "0         101        4975\n",
      "1         102        1200\n",
      "2         103         380\n",
      "3         104         104\n",
      "4         105         320\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "grp_df = df.groupby('CustomerID')['TotalPrice'].sum().reset_index()\n",
    "print(\"Grouped DataFrame by CustomerID:\")\n",
    "print(grp_df)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab7324",
   "metadata": {},
   "source": [
    "## Question 5: Sorting Data\n",
    "\n",
    "Task: Sort the DataFrame first by OrderDate in ascending order, and then by TotalPrice in descending order.\n",
    "\n",
    "Expected Output: Sorted DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2621c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted DataFrame:\n",
      "    OrderID  CustomerID  OrderDate   Product  Quantity  Price  TotalPrice\n",
      "0         1         101 2024-01-15    Laptop         1   1200        1200\n",
      "10        1         101 2024-01-15    Laptop         1   1200        1200\n",
      "1         2         102 2024-01-20     Mouse         2     25          50\n",
      "2         3         101 2024-02-01  Keyboard         1     75          75\n",
      "3         4         103 2024-02-05   Monitor         1    300         300\n",
      "4         5         102 2024-02-10    Laptop         1   1150        1150\n",
      "5         6         104 2024-03-01     Mouse         3     20          60\n",
      "6         7         101 2024-03-05    Laptop         2   1250        2500\n",
      "7         8         103 2024-03-10  Keyboard         1     80          80\n",
      "8         9         105 2024-03-15   Monitor         1    320         320\n",
      "9        10         104 2024-04-01     Mouse         2     22          44\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sorted_df = df.sort_values(by=['OrderDate', 'TotalPrice'], ascending=[True, False])\n",
    "print(\"Sorted DataFrame:\")\n",
    "print(sorted_df)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c0758",
   "metadata": {},
   "source": [
    "## Question 6: Handling Duplicates (Conceptual & Practical)\n",
    "\n",
    "Task:\n",
    "\n",
    "Conceptual: If you had duplicate rows (e.g., identical OrderID, CustomerID, Product, etc.), how would you identify them?\n",
    "\n",
    "Practical: (No actual duplicates in our sample data, but assume you need to demonstrate the method) Write code to remove duplicates based on OrderID, keeping the first occurrence.\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "Conceptual explanation.\n",
    "\n",
    "DataFrame with duplicates removed (if any were present)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c5d25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after dropping duplicates:\n",
      "   OrderID  CustomerID  OrderDate   Product  Quantity  Price  TotalPrice\n",
      "0        1         101 2024-01-15    Laptop         1   1200        1200\n",
      "1        2         102 2024-01-20     Mouse         2     25          50\n",
      "2        3         101 2024-02-01  Keyboard         1     75          75\n",
      "3        4         103 2024-02-05   Monitor         1    300         300\n",
      "4        5         102 2024-02-10    Laptop         1   1150        1150\n",
      "5        6         104 2024-03-01     Mouse         3     20          60\n",
      "6        7         101 2024-03-05    Laptop         2   1250        2500\n",
      "7        8         103 2024-03-10  Keyboard         1     80          80\n",
      "8        9         105 2024-03-15   Monitor         1    320         320\n",
      "9       10         104 2024-04-01     Mouse         2     22          44\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "drop_duplicates_df = df.drop_duplicates(subset=['OrderID'], keep='first')\n",
    "print(\"DataFrame after dropping duplicates:\")\n",
    "print(drop_duplicates_df)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fa8ba",
   "metadata": {},
   "source": [
    "## Question 7: Time-based Filtering\n",
    "\n",
    "Task: Filter the DataFrame to show all orders placed in the month of March 2024.\n",
    "\n",
    "Expected Output: Filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ae62f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame by OrderDate:\n",
      "   OrderID  CustomerID  OrderDate   Product  Quantity  Price  TotalPrice\n",
      "5        6         104 2024-03-01     Mouse         3     20          60\n",
      "6        7         101 2024-03-05    Laptop         2   1250        2500\n",
      "7        8         103 2024-03-10  Keyboard         1     80          80\n",
      "8        9         105 2024-03-15   Monitor         1    320         320\n"
     ]
    }
   ],
   "source": [
    "filter_order_df = df[(df['OrderDate'] >= '2024-03-01') & (df['OrderDate'] < '2024-04-01')]\n",
    "print(\"Filtered DataFrame by OrderDate:\")\n",
    "print(filter_order_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04589d1",
   "metadata": {},
   "source": [
    "## Question 8: Conditional Value Assignment (.loc or np.where)\n",
    "\n",
    "Task: Create a new column called OrderCategory. If TotalPrice is greater than 500, assign 'High Value', otherwise assign 'Standard Value'.\n",
    "\n",
    "Expected Output: DataFrame with the new OrderCategory column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d19b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with OrderCategory column:\n",
      "    OrderID  CustomerID  OrderDate   Product  Quantity  Price  TotalPrice  \\\n",
      "0         1         101 2024-01-15    Laptop         1   1200        1200   \n",
      "1         2         102 2024-01-20     Mouse         2     25          50   \n",
      "2         3         101 2024-02-01  Keyboard         1     75          75   \n",
      "3         4         103 2024-02-05   Monitor         1    300         300   \n",
      "4         5         102 2024-02-10    Laptop         1   1150        1150   \n",
      "5         6         104 2024-03-01     Mouse         3     20          60   \n",
      "6         7         101 2024-03-05    Laptop         2   1250        2500   \n",
      "7         8         103 2024-03-10  Keyboard         1     80          80   \n",
      "8         9         105 2024-03-15   Monitor         1    320         320   \n",
      "9        10         104 2024-04-01     Mouse         2     22          44   \n",
      "10        1         101 2024-01-15    Laptop         1   1200        1200   \n",
      "\n",
      "     OrderCategory  \n",
      "0       High Value  \n",
      "1   Standard Value  \n",
      "2   Standard Value  \n",
      "3   Standard Value  \n",
      "4       High Value  \n",
      "5   Standard Value  \n",
      "6       High Value  \n",
      "7   Standard Value  \n",
      "8   Standard Value  \n",
      "9   Standard Value  \n",
      "10      High Value  \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "df['OrderCategory'] = np.where(df['TotalPrice'] > 500, 'High Value', 'Standard Value')\n",
    "print(\"DataFrame with OrderCategory column:\")\n",
    "print(df)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d23ab",
   "metadata": {},
   "source": [
    "## Question 9: Pivot Table / Crosstab (Conceptual)\n",
    "\n",
    "Task: How would you use Pandas to create a table showing the total Quantity sold for each Product across different OrderCategorys (from Q8)?\n",
    "\n",
    "Expected Output: Conceptual explanation using pivot_table or groupby and unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caf4f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot Table:\n",
      "OrderCategory  High Value  Standard Value\n",
      "Product                                  \n",
      "Keyboard                0               2\n",
      "Laptop                  5               0\n",
      "Monitor                 0               2\n",
      "Mouse                   0               7\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "pivot_df = df.pivot_table(index='Product', columns='OrderCategory', values='Quantity', aggfunc='sum', fill_value=0)\n",
    "print(\"Pivot Table:\")\n",
    "print(pivot_df)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a3cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: df_orders\n",
      "   OrderID  CustomerID  OrderDate Product  Quantity  PricePerUnit  TotalAmount\n",
      "0      101           1 2023-01-05       A         2          50.0        100.0\n",
      "1      102           2 2023-01-10       B         1          75.0         75.0\n",
      "2      103           1 2023-01-15       A         3          50.0        150.0\n",
      "3      104           3 2023-01-20       C         1         100.0        100.0\n",
      "4      105           2 2023-02-01       B         2          75.0        150.0\n",
      "5      106           4 2023-02-05       D         1         120.0        120.0\n",
      "6      107           1 2023-02-10       A         1          50.0         50.0\n",
      "7      108           3 2023-02-15       C         2         100.0        200.0\n",
      "8      109           5 2023-03-01       E         1          90.0         90.0\n",
      "9      110           4 2023-03-05       D         3         120.0        360.0\n",
      "--------------------------------------------------\n",
      "DataFrame: df_customers\n",
      "   CustomerID     Name         City   JoinDate\n",
      "0           1    Alice     New York 2022-01-01\n",
      "1           2      Bob  Los Angeles 2022-03-15\n",
      "2           3  Charlie      Chicago 2022-05-01\n",
      "3           4    David     New York 2022-07-20\n",
      "4           5      Eve  Los Angeles 2022-09-10\n",
      "5           6    Frank      Houston 2022-11-01\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "orders_data = {\n",
    "    'OrderID': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "    'CustomerID': [1, 2, 1, 3, 2, 4, 1, 3, 5, 4],\n",
    "    'OrderDate': pd.to_datetime(['2023-01-05', '2023-01-10', '2023-01-15', '2023-01-20', '2023-02-01',\n",
    "                                  '2023-02-05', '2023-02-10', '2023-02-15', '2023-03-01', '2023-03-05']),\n",
    "    'Product': ['A', 'B', 'A', 'C', 'B', 'D', 'A', 'C', 'E', 'D'],\n",
    "    'Quantity': [2, 1, 3, 1, 2, 1, 1, 2, 1, 3],\n",
    "    'PricePerUnit': [50.0, 75.0, 50.0, 100.0, 75.0, 120.0, 50.0, 100.0, 90.0, 120.0]\n",
    "}\n",
    "df_orders = pd.DataFrame(orders_data)\n",
    "df_orders['TotalAmount'] = df_orders['Quantity'] * df_orders['PricePerUnit']\n",
    "\n",
    "print(\"DataFrame: df_orders\")\n",
    "print(df_orders)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "customers_data = {\n",
    "    'CustomerID': [1, 2, 3, 4, 5, 6],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank'],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'New York', 'Los Angeles', 'Houston'],\n",
    "    'JoinDate': pd.to_datetime(['2022-01-01', '2022-03-15', '2022-05-01', '2022-07-20', '2022-09-10', '2022-11-01'])\n",
    "}\n",
    "df_customers = pd.DataFrame(customers_data)\n",
    "\n",
    "print(\"DataFrame: df_customers\")\n",
    "print(df_customers)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64841cbd",
   "metadata": {},
   "source": [
    "## Question 1: Merging DataFrames\n",
    "\n",
    "Task: Merge df_orders and df_customers to include customer Name and City in the orders DataFrame. Perform an inner join.\n",
    "\n",
    "Expected Output: A new DataFrame df_merged_orders with OrderID, CustomerID, OrderDate, Product, Quantity, PricePerUnit, TotalAmount, Name, and City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d021b2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame:\n",
      "   OrderID  CustomerID  OrderDate Product  Quantity  PricePerUnit  \\\n",
      "0      101           1 2023-01-05       A         2          50.0   \n",
      "1      102           2 2023-01-10       B         1          75.0   \n",
      "2      103           1 2023-01-15       A         3          50.0   \n",
      "3      104           3 2023-01-20       C         1         100.0   \n",
      "4      105           2 2023-02-01       B         2          75.0   \n",
      "5      106           4 2023-02-05       D         1         120.0   \n",
      "6      107           1 2023-02-10       A         1          50.0   \n",
      "7      108           3 2023-02-15       C         2         100.0   \n",
      "8      109           5 2023-03-01       E         1          90.0   \n",
      "9      110           4 2023-03-05       D         3         120.0   \n",
      "\n",
      "   TotalAmount     Name         City  \n",
      "0        100.0    Alice     New York  \n",
      "1         75.0      Bob  Los Angeles  \n",
      "2        150.0    Alice     New York  \n",
      "3        100.0  Charlie      Chicago  \n",
      "4        150.0      Bob  Los Angeles  \n",
      "5        120.0    David     New York  \n",
      "6         50.0    Alice     New York  \n",
      "7        200.0  Charlie      Chicago  \n",
      "8         90.0      Eve  Los Angeles  \n",
      "9        360.0    David     New York  \n"
     ]
    }
   ],
   "source": [
    "df_merged = pd.merge(df_orders, df_customers[['CustomerID', 'Name','City']], on = 'CustomerID', how = 'left')\n",
    "print(\"Merged DataFrame:\")\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c4340",
   "metadata": {},
   "source": [
    "## Question 2: Handling Missing Data (Simulation & Imputation)\n",
    "\n",
    "Task: Simulate Missing Data: Create a copy of df_orders. In this copy, randomly set 3 PricePerUnit values to np.nan.\n",
    "\n",
    "Impute Missing Data: Fill these missing PricePerUnit values with the mean PricePerUnit of that specific Product. If a product has no previous price to calculate the mean, use the overall mean of the PricePerUnit column.\n",
    "\n",
    "Expected Output: The modified DataFrame with np.nan values replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f6da9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OrderID  CustomerID  OrderDate Product  Quantity  PricePerUnit  TotalAmount\n",
      "0      101           1 2023-01-05       A         2          50.0        100.0\n",
      "1      102           2 2023-01-10       B         1           NaN         75.0\n",
      "2      103           1 2023-01-15       A         3          50.0        150.0\n",
      "3      104           3 2023-01-20       C         1         100.0        100.0\n",
      "4      105           2 2023-02-01       B         2          75.0        150.0\n",
      "5      106           4 2023-02-05       D         1           NaN        120.0\n",
      "6      107           1 2023-02-10       A         1          50.0         50.0\n",
      "7      108           3 2023-02-15       C         2         100.0        200.0\n",
      "8      109           5 2023-03-01       E         1           NaN         90.0\n",
      "9      110           4 2023-03-05       D         3         120.0        360.0\n",
      "   OrderID  CustomerID  OrderDate Product  Quantity  PricePerUnit  TotalAmount\n",
      "0      101           1 2023-01-05       A         2     50.000000        100.0\n",
      "1      102           2 2023-01-10       B         1     75.000000         75.0\n",
      "2      103           1 2023-01-15       A         3     50.000000        150.0\n",
      "3      104           3 2023-01-20       C         1    100.000000        100.0\n",
      "4      105           2 2023-02-01       B         2     75.000000        150.0\n",
      "5      106           4 2023-02-05       D         1    120.000000        120.0\n",
      "6      107           1 2023-02-10       A         1     50.000000         50.0\n",
      "7      108           3 2023-02-15       C         2    100.000000        200.0\n",
      "8      109           5 2023-03-01       E         1     77.857143         90.0\n",
      "9      110           4 2023-03-05       D         3    120.000000        360.0\n"
     ]
    }
   ],
   "source": [
    "df_orders_copy = df_orders.copy()\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(df_orders_copy.index, size=3, replace=False)\n",
    "df_orders_copy.loc[random_indices, 'PricePerUnit'] = np.nan\n",
    "\n",
    "print(df_orders_copy)\n",
    "\n",
    "product_means = df_orders_copy.groupby('Product')['PricePerUnit'].transform(lambda x: x.fillna(x.mean()))\n",
    "# Fill NaNs with product-specific mean, then fill remaining (if product had no valid prices) with overall mean\n",
    "df_orders_copy['PricePerUnit'] = product_means.fillna(df_orders_copy['PricePerUnit'].mean())\n",
    "\n",
    "product_means\n",
    "print(df_orders_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdf3d7",
   "metadata": {},
   "source": [
    "## Question 3: Top N Analysis\n",
    "\n",
    "Task: Find the top 3 Products by total Quantity sold.\n",
    "\n",
    "Expected Output: A DataFrame or Series with the top 3 products and their total quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26643858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped DataFrame by Product:\n",
      "  Product  Quantity\n",
      "0       A         6\n",
      "1       D         4\n",
      "2       B         3\n"
     ]
    }
   ],
   "source": [
    "grp_product_df = df_merged.groupby('Product')['Quantity'].sum().nlargest(3).reset_index()\n",
    "\n",
    "print(\"Grouped DataFrame by Product:\")\n",
    "print(grp_product_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91af2a0",
   "metadata": {},
   "source": [
    "## Question 4: Advanced Grouping and Aggregation\n",
    "\n",
    "Task:\n",
    "\n",
    "Calculate the total TotalAmount and average Quantity for each City.\n",
    "\n",
    "For the same aggregation, also find the number of unique Products sold in each city.\n",
    "\n",
    "Expected Output: A DataFrame grouped by City showing aggregated TotalAmount, Average Quantity, and Unique Products Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0276a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped DataFrame by City:\n",
      "          City  Total_Sales  Average_Quantity  Unique_Products_Count\n",
      "0      Chicago        300.0          1.500000                      1\n",
      "1  Los Angeles        315.0          1.333333                      2\n",
      "2     New York        780.0          2.000000                      2\n"
     ]
    }
   ],
   "source": [
    "grp_df = df_merged.groupby('City').agg(Total_Sales=('TotalAmount', 'sum'),\n",
    "    Average_Quantity=('Quantity', 'mean'),\n",
    "    Unique_Products_Count=('Product', 'nunique')).reset_index()\n",
    "print(\"Grouped DataFrame by City:\")\n",
    "print(grp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b974c0bb",
   "metadata": {},
   "source": [
    "## Question 5: Time-based Analysis (Monthly Sales)\n",
    "\n",
    "Task: Calculate the total TotalAmount for each month of the year (based on OrderDate).\n",
    "\n",
    "Expected Output: A Series or DataFrame showing the month (e.g., '2023-01', '2023-02', etc.) and the corresponding total TotalAmount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82f04cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sales by Month:\n",
      "OrderMonth\n",
      "2023-01    425.0\n",
      "2023-02    520.0\n",
      "2023-03    450.0\n",
      "Freq: M, Name: TotalAmount, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_orders['OrderMonth'] = df_orders['OrderDate'].dt.to_period('M')\n",
    "\n",
    "total_sales_month = df_orders.groupby('OrderMonth')['TotalAmount'].sum()\n",
    "print(\"Total Sales by Month:\")\n",
    "print(total_sales_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4364b45",
   "metadata": {},
   "source": [
    "# Sci-Kit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37edc102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, roc_auc_score, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6083d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Features\n",
    "age = np.random.randint(20, 60, n_samples)\n",
    "salary = np.random.randint(30000, 120000, n_samples)\n",
    "education_level = np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples)\n",
    "experience = np.random.randint(1, 15, n_samples)\n",
    "city = np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami'], n_samples)\n",
    "has_loan = np.random.choice([0, 1], n_samples, p=[0.7, 0.3]) # Binary categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6237b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target for Classification (e.g., Customer Churn: 0 = No, 1 = Yes)\n",
    "# Churn is more likely for older, lower salary, less education, high loan\n",
    "churn_prob = (\n",
    "    0.1 * (age / 60) +\n",
    "    0.05 * (1 - salary / 120000) +\n",
    "    0.08 * (education_level == 'High School') +\n",
    "    0.07 * has_loan +\n",
    "    np.random.rand(n_samples) * 0.1 # Noise\n",
    ")\n",
    "churn = (churn_prob > 0.15).astype(int)\n",
    "\n",
    "# Target for Regression (e.g., Customer Spending)\n",
    "# Spending is higher for higher salary, more education, more experience\n",
    "spending = (\n",
    "    5000 +\n",
    "    2 * salary / 1000 +\n",
    "    1000 * (education_level == 'Bachelor') +\n",
    "    2000 * (education_level == 'Master') +\n",
    "    3000 * (education_level == 'PhD') +\n",
    "    200 * experience +\n",
    "    np.random.randn(n_samples) * 1000 # Noise\n",
    ")\n",
    "spending = np.maximum(0, spending) # Ensure no negative spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b021b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of the Dataset:\n",
      "   Age  Salary    Education  Experience         City  HasLoan  Churn  \\\n",
      "0   58   81934     Bachelor          14  Los Angeles        0      1   \n",
      "1   48   94895  High School           5      Houston        0      1   \n",
      "2   34   62307       Master           7  Los Angeles        0      0   \n",
      "3   27   84098       Master           9      Houston        0      0   \n",
      "4   40   90921          PhD           3      Houston        0      0   \n",
      "\n",
      "       Spending  \n",
      "0   8854.759482  \n",
      "1   5701.192418  \n",
      "2   8255.619493  \n",
      "3  10533.367933  \n",
      "4   7915.975563  \n",
      "(500, 8)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Salary': salary,\n",
    "    'Education': education_level,\n",
    "    'Experience': experience,\n",
    "    'City': city,\n",
    "    'HasLoan': has_loan,\n",
    "    'Churn': churn, # Classification target\n",
    "    'Spending': spending # Regression target\n",
    "})\n",
    "\n",
    "print(\"Sample of the Dataset:\")\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ae25f",
   "metadata": {},
   "source": [
    "**Question 1: Data Splitting**\n",
    "\n",
    "* **Task:** Split the `data_sk` DataFrame into features (X) and target (y) for the **classification task (`Churn`)**. Then, divide this data into training and testing sets, with 80% for training and 20% for testing. Ensure reproducibility (`random_state=42`) and stratified splitting to maintain class distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d82558d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((400, 7), (100, 7), (400,), (100,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop('Churn', axis=1),\n",
    "    data['Churn'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ed9e6",
   "metadata": {},
   "source": [
    "**Question 2: Preprocessing - Numerical Features**\n",
    "\n",
    "* **Task:** For the classification task, identify all numerical features (e.g., `Age`, `Salary`, `Experience`). Create a `StandardScaler` and fit it only on the training data. Transform both the training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eebcd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11936954  0.05974987  0.8066582 ]\n",
      " [ 1.17111196  0.02050751 -1.37023049]\n",
      " [ 1.42920826 -1.17283204 -1.61210701]\n",
      " [ 1.60127245 -1.54128032  1.04853472]\n",
      " [-1.23778683  1.41337462  0.56478167]]\n",
      "[[ 0.74095146  1.23108163 -0.64460093]\n",
      " [ 0.39682306 -1.1020719   0.08102863]\n",
      " [ 0.13872676 -0.07051146 -0.40272441]\n",
      " [ 1.51524035  1.06073989  1.29041124]\n",
      " [ 1.60127245 -0.07040215  1.04853472]]\n"
     ]
    }
   ],
   "source": [
    "numerical_cols = ['Age', 'Salary', 'Experience']\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test_scaled = scaler.transform(X_test[numerical_cols])\n",
    "\n",
    "print(X_train_scaled[:5])\n",
    "print(X_test_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f320b",
   "metadata": {},
   "source": [
    "**Question 3: Preprocessing - Categorical Features (One-Hot Encoding)**\n",
    "\n",
    "* **Task:** For the classification task, identify the categorical features (`Education`, `City`). Apply `OneHotEncoder` to these features. Fit the encoder only on the training data and transform both training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0cef6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['Education', 'City']\n",
    "one_code = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_train_encoded = one_code.fit_transform(X_train[categorical_cols])\n",
    "X_test_encoded = one_code.transform(X_test[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef0f80",
   "metadata": {},
   "source": [
    "**Question 4: Full Preprocessing Pipeline with `ColumnTransformer`**\n",
    "\n",
    "* **Task:** Combine the numerical scaling and categorical one-hot encoding into a single `ColumnTransformer`. Apply this preprocessor to the training and testing sets (`X_train_cls`, `X_test_cls`) for the classification task.\n",
    "    * Numerical features: `Age`, `Salary`, `Experience`.\n",
    "    * Categorical features: `Education`, `City`.\n",
    "    * Features to pass through without transformation: `HasLoan`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8035af5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11936954  0.05974987  0.8066582   0.          0.          0.\n",
      "   1.          1.          0.          0.          0.          0.\n",
      "   0.        ]\n",
      " [ 1.17111196  0.02050751 -1.37023049  0.          1.          0.\n",
      "   0.          0.          1.          0.          0.          0.\n",
      "   1.        ]\n",
      " [ 1.42920826 -1.17283204 -1.61210701  0.          0.          1.\n",
      "   0.          0.          0.          0.          1.          0.\n",
      "   0.        ]\n",
      " [ 1.60127245 -1.54128032  1.04853472  1.          0.          0.\n",
      "   0.          0.          0.          1.          0.          0.\n",
      "   0.        ]\n",
      " [-1.23778683  1.41337462  0.56478167  0.          0.          1.\n",
      "   0.          0.          0.          1.          0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "numerical_features_cls = ['Age', 'Salary', 'Experience']\n",
    "categorical_features_cls = ['Education', 'City']\n",
    "passthrough_features_cls = ['HasLoan']\n",
    "\n",
    "preprocessor_cls = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features_cls),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features_cls),\n",
    "        ('pass', 'passthrough', passthrough_features_cls)\n",
    "    ],\n",
    "    remainder='drop' # Drop any columns not specified\n",
    ")\n",
    "\n",
    "X_train_cls_processed = preprocessor_cls.fit_transform(X_train)\n",
    "X_test_cls_processed = preprocessor_cls.transform(X_test)\n",
    "\n",
    "print(X_train_cls_processed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d74e2",
   "metadata": {},
   "source": [
    "**Question 5: Building a Classification Model**\n",
    "\n",
    "* **Task:** Using the preprocessed data from Q4, train a `LogisticRegression` model to predict `Churn`. Use default parameters. Evaluate its `accuracy_score` and `roc_auc_score` on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e81ecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.71      0.75        31\n",
      "           1       0.88      0.91      0.89        69\n",
      "\n",
      "    accuracy                           0.85       100\n",
      "   macro avg       0.83      0.81      0.82       100\n",
      "weighted avg       0.85      0.85      0.85       100\n",
      "\n",
      "ROC AUC Score: 0.8113604488078543\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression(random_state=42, solver='liblinear') # Using liblinear for robustness\n",
    "model_lr.fit(X_train_cls_processed, y_train)\n",
    "\n",
    "y_pred_lr = model_lr.predict(X_test_cls_processed)\n",
    "y_proba_lr = model_lr.predict_proba(X_test_cls_processed)[:, 1]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr)}\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_lr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debab539",
   "metadata": {},
   "source": [
    "**Question 6: Building a Regression Model**\n",
    "\n",
    "* **Task:** Now switch to the **regression task (`Spending`)**. Perform a train-test split (80/20). Using the same `ColumnTransformer` setup as in Q4 (fit on the new training data), preprocess the features. Train a `LinearRegression` model. Evaluate its `mean_squared_error` and `r2_score` on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea2f8436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1151586.5375877235\n",
      "R2 Score Error: 0.598796405302632\n"
     ]
    }
   ],
   "source": [
    "X_reg = data.drop('Spending', axis=1)\n",
    "y_reg = data['Spending']\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "preprocessor_reg = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['Age', 'Salary', 'Experience']),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['Education', 'City']),\n",
    "        ('pass', 'passthrough', ['HasLoan'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "X_train_reg_processed = preprocessor_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_processed = preprocessor_reg.transform(X_test_reg)\n",
    "\n",
    "model_lr_reg = LinearRegression()\n",
    "model_lr_reg.fit(X_train_reg_processed, y_train_reg)\n",
    "y_pred_reg = model_lr_reg.predict(X_test_reg_processed)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(y_test_reg, y_pred_reg)}\")\n",
    "print(f\"R2 Score Error: {r2_score(y_test_reg, y_pred_reg)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6e7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
